# Kubernetes Cluster Setup with kubeadm

This guide explains how to set up a Kubernetes cluster from scratch using **kubeadm**, **containerd**, and **Flannel** as the CNI (Container Network Interface).

---

## üñ•Ô∏è Prerequisites

* Multiple Linux nodes (Ubuntu recommended)
* One control plane (master) node and one or more worker nodes
* `sudo` privileges on all nodes
* Internet access for downloading packages

---
## üì¶ VM Provisioning with Vagrant + VirtualBox

If you don‚Äôt want to prepare physical machines or cloud VMs manually, you can use **Vagrant** with **VirtualBox** to spin up the cluster nodes.

1. Clone this repository:
   ```bash
   git clone <repo-url>
   cd <repo-name>
```
## Inspect the included Vagrantfile and adjust CPU, memory, or network settings as needed.

## Bring up the VMs
```bash
vagrant up
```
## Check the status
```bash
vagrant status
```
## SSH into the nodes:
```bash
vagrant ssh controlplane
vagrant ssh node01
vagrant ssh node02
```

## 1. Install Kubernetes Components (On Every Node)

```bash
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Install kubelet, kubeadm, and kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
kubeadm version
```

---

## 2. Install and Configure containerd (On Every Node)

```bash
sudo apt install containerd -y
sudo mkdir -p /etc/containerd

# Generate default config and enable systemd cgroup driver
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml

# Verify
cat /etc/containerd/config.toml | grep -i SystemdCgroup

# Restart containerd
sudo systemctl restart containerd
```

---

## 3. Enable IPv4 Packet Forwarding (On Every Node)

```bash
# Load module now
sudo modprobe br_netfilter

# Make sure it loads on boot
echo "br_netfilter" | sudo tee /etc/modules-load.d/br_netfilter.conf

# Add missing sysctl params
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

# Apply immediately
sudo sysctl --system

```

---

## 4. Configure Kubelet Cgroup Driver (On Every Node)

Check if the system is using `systemd`:

```bash
ps -p 1
```

If output is `systemd`, then kubeadm v1.22+ automatically defaults to the systemd cgroup driver ‚úÖ No need to do anything about it.

---
---
Disacle Swap Memory
```bash
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```
---

## 5. Initialize the Cluster (On Control Plane Node)

```bash
sudo kubeadm init \
  --apiserver-advertise-address <master_node_ip> \
  --pod-network-cidr "10.244.0.0/16" \
  --upload-certs
```

Set up kubeconfig:

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

---
## 6. Install Network Add-on (Calico)
# Run this on the control plane node:
 *before apply the manifest make sure you will set the default pod-cidr as 10.244.0.0/16. Calico will get the cidr pool automatically from kube-api-server which is running on kube-system namespace.Else you can set the cidr manually in the calico.yaml in here which lives in the calico-node DaemonSet under:
```bash
wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml
```
```bash
- name: CALICO_IPV4POOL_CIDR
  value: "10.244.0.0/16"
```
* if you set the default cidr then just apply the manifest.
```bash 
kubectl apply -f calico.yaml
kubectl get pods -n kube-system -w
```
## OR Install Network Add-on (Flannel)

```bash
wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
kubectl apply -f kube-flannel.yml

# Verify flannel pods
kubectl get po -n kube-flannel
```

> ‚ö†Ô∏è Make sure the CIDR block in `kube-flannel.yml` matches `10.244.0.0/16`.
---

## 7. Join Worker Nodes (On Each Worker Node)

Run the join command generated by `kubeadm init`. You can find that command after the cluster initialization. 

```bash
kubeadm join <master_node_IP>:6443 --token <token> \
  --discovery-token-ca-cert-hash <hash_token>
```

---

## 8. Label Worker Nodes (On Control Plane)

```bash
kubectl label node node01 node-role.kubernetes.io/worker=worker1
kubectl label node node02 node-role.kubernetes.io/worker=worker2

# Verify
kubectl get nodes -o wide
```
If you already ran kubeadm init on your control-plane node, and forget to save the 'kubeadm join' command, you can print the kubeadm join command again with:
```bash
kubeadm token create --print-join-command
```
---

## ‚úÖ Done!

You now have a working Kubernetes cluster with containerd and Flannel networking.
## When you‚Äôre done experimenting, destroy all VMs with a single command:
```bash
vagrant logout #run on each node
vagrant destroy -f
```








